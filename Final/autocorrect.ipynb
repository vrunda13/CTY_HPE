{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "check = Speller(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doesthis sentence ha ve misspelled words?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check(\"doesthis sentece ha ve misspelled wordz?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '1080', '&c', '10-point', '10th']\n",
      "['jigaboo', 'mound of venus', 'asslover', 's&m', 'queaf']\n",
      "Total english words: 466544\n",
      "Total bad words: 1617\n",
      "create symspell dict...\n"
     ]
    }
   ],
   "source": [
    "import re, random\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "to_sample = False # if you're impatient switch this flag\n",
    "\n",
    "def spacy_tokenize(text):\n",
    "    return [token.text for token in nlp.tokenizer(text)]\n",
    "    \n",
    "def dameraulevenshtein(seq1, seq2):\n",
    "    \"\"\"Calculate the Damerau-Levenshtein distance between sequences.\n",
    "    This method has not been modified from the original.\n",
    "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
    "    This distance is the number of additions, deletions, substitutions,\n",
    "    and transpositions needed to transform the first sequence into the\n",
    "    second. Although generally used with strings, any sequences of\n",
    "    comparable objects will work.\n",
    "    Transpositions are exchanges of *consecutive* characters; all other\n",
    "    operations are self-explanatory.\n",
    "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
    "    lengths of the two sequences.\n",
    "    >>> dameraulevenshtein('ba', 'abc')\n",
    "    2\n",
    "    >>> dameraulevenshtein('fee', 'deed')\n",
    "    2\n",
    "    It works with arbitrary sequences too:\n",
    "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
    "    2\n",
    "    \"\"\"\n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
    "    # However, only the current and two previous rows are needed at once,\n",
    "    # so we only store those.\n",
    "    oneago = None\n",
    "    thisrow = list(range(1, len(seq2) + 1)) + [0]\n",
    "    for x in range(len(seq1)):\n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = (oneago, thisrow, [0] * len(seq2) + [x + 1])\n",
    "        for y in range(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                    and seq1[x - 1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "\n",
    "class SymSpell:\n",
    "    def __init__(self, max_edit_distance=3, verbose=0):\n",
    "        self.max_edit_distance = max_edit_distance\n",
    "        self.verbose = verbose\n",
    "        # 0: top suggestion\n",
    "        # 1: all suggestions of smallest edit distance\n",
    "        # 2: all suggestions <= max_edit_distance (slower, no early termination)\n",
    "\n",
    "        self.dictionary = {}\n",
    "        self.longest_word_length = 0\n",
    "\n",
    "    def get_deletes_list(self, w):\n",
    "        \"\"\"given a word, derive strings with up to max_edit_distance characters\n",
    "           deleted\"\"\"\n",
    "\n",
    "        deletes = []\n",
    "        queue = [w]\n",
    "        for d in range(self.max_edit_distance):\n",
    "            temp_queue = []\n",
    "            for word in queue:\n",
    "                if len(word) > 1:\n",
    "                    for c in range(len(word)):  # character index\n",
    "                        word_minus_c = word[:c] + word[c + 1:]\n",
    "                        if word_minus_c not in deletes:\n",
    "                            deletes.append(word_minus_c)\n",
    "                        if word_minus_c not in temp_queue:\n",
    "                            temp_queue.append(word_minus_c)\n",
    "            queue = temp_queue\n",
    "\n",
    "        return deletes\n",
    "\n",
    "    def create_dictionary_entry(self, w):\n",
    "        '''add word and its derived deletions to dictionary'''\n",
    "        # check if word is already in dictionary\n",
    "        # dictionary entries are in the form: (list of suggested corrections,\n",
    "        # frequency of word in corpus)\n",
    "        new_real_word_added = False\n",
    "        if w in self.dictionary:\n",
    "            # increment count of word in corpus\n",
    "            self.dictionary[w] = (self.dictionary[w][0], self.dictionary[w][1] + 1)\n",
    "        else:\n",
    "            self.dictionary[w] = ([], 1)\n",
    "            self.longest_word_length = max(self.longest_word_length, len(w))\n",
    "\n",
    "        if self.dictionary[w][1] == 1:\n",
    "            # first appearance of word in corpus\n",
    "            # n.b. word may already be in dictionary as a derived word\n",
    "            # (deleting character from a real word)\n",
    "            # but counter of frequency of word in corpus is not incremented\n",
    "            # in those cases)\n",
    "            new_real_word_added = True\n",
    "            deletes = self.get_deletes_list(w)\n",
    "            for item in deletes:\n",
    "                if item in self.dictionary:\n",
    "                    # add (correct) word to delete's suggested correction list\n",
    "                    self.dictionary[item][0].append(w)\n",
    "                else:\n",
    "                    # note frequency of word in corpus is not incremented\n",
    "                    self.dictionary[item] = ([w], 0)\n",
    "\n",
    "        return new_real_word_added\n",
    "\n",
    "    def create_dictionary_from_arr(self, arr, token_pattern=r'[a-z]+'):\n",
    "        total_word_count = 0\n",
    "        unique_word_count = 0\n",
    "\n",
    "        for line in arr:\n",
    "            # separate by words by non-alphabetical characters\n",
    "            words = re.findall(token_pattern, line.lower())\n",
    "            for word in words:\n",
    "                total_word_count += 1\n",
    "                if self.create_dictionary_entry(word):\n",
    "                    unique_word_count += 1\n",
    "\n",
    "        print(\"total words processed: %i\" % total_word_count)\n",
    "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
    "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
    "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
    "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
    "        return self.dictionary\n",
    "\n",
    "    def create_dictionary(self, fname):\n",
    "        total_word_count = 0\n",
    "        unique_word_count = 0\n",
    "\n",
    "        with open(fname) as file:\n",
    "            for line in file:\n",
    "                # separate by words by non-alphabetical characters\n",
    "                words = re.findall('[a-z]+', line.lower())\n",
    "                for word in words:\n",
    "                    total_word_count += 1\n",
    "                    if self.create_dictionary_entry(word):\n",
    "                        unique_word_count += 1\n",
    "\n",
    "        print(\"total words processed: %i\" % total_word_count)\n",
    "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
    "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
    "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
    "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
    "        return self.dictionary\n",
    "\n",
    "    def get_suggestions(self, string, silent=False):\n",
    "        \"\"\"return list of suggested corrections for potentially incorrectly\n",
    "           spelled word\"\"\"\n",
    "        if (len(string) - self.longest_word_length) > self.max_edit_distance:\n",
    "            if not silent:\n",
    "                print(\"no items in dictionary within maximum edit distance\")\n",
    "            return []\n",
    "\n",
    "        suggest_dict = {}\n",
    "        min_suggest_len = float('inf')\n",
    "\n",
    "        queue = [string]\n",
    "        q_dictionary = {}  # items other than string that we've checked\n",
    "\n",
    "        while len(queue) > 0:\n",
    "            q_item = queue[0]  # pop\n",
    "            queue = queue[1:]\n",
    "\n",
    "            # early exit\n",
    "            if ((self.verbose < 2) and (len(suggest_dict) > 0) and\n",
    "                    ((len(string) - len(q_item)) > min_suggest_len)):\n",
    "                break\n",
    "\n",
    "            # process queue item\n",
    "            if (q_item in self.dictionary) and (q_item not in suggest_dict):\n",
    "                if self.dictionary[q_item][1] > 0:\n",
    "                    # word is in dictionary, and is a word from the corpus, and\n",
    "                    # not already in suggestion list so add to suggestion\n",
    "                    # dictionary, indexed by the word with value (frequency in\n",
    "                    # corpus, edit distance)\n",
    "                    # note q_items that are not the input string are shorter\n",
    "                    # than input string since only deletes are added (unless\n",
    "                    # manual dictionary corrections are added)\n",
    "                    assert len(string) >= len(q_item)\n",
    "                    suggest_dict[q_item] = (self.dictionary[q_item][1],\n",
    "                                            len(string) - len(q_item))\n",
    "                    # early exit\n",
    "                    if (self.verbose < 2) and (len(string) == len(q_item)):\n",
    "                        break\n",
    "                    elif (len(string) - len(q_item)) < min_suggest_len:\n",
    "                        min_suggest_len = len(string) - len(q_item)\n",
    "\n",
    "                # the suggested corrections for q_item as stored in\n",
    "                # dictionary (whether or not q_item itself is a valid word\n",
    "                # or merely a delete) can be valid corrections\n",
    "                for sc_item in self.dictionary[q_item][0]:\n",
    "                    if sc_item not in suggest_dict:\n",
    "\n",
    "                        # compute edit distance\n",
    "                        # suggested items should always be longer\n",
    "                        # (unless manual corrections are added)\n",
    "                        assert len(sc_item) > len(q_item)\n",
    "\n",
    "                        # q_items that are not input should be shorter\n",
    "                        # than original string\n",
    "                        # (unless manual corrections added)\n",
    "                        assert len(q_item) <= len(string)\n",
    "\n",
    "                        if len(q_item) == len(string):\n",
    "                            assert q_item == string\n",
    "                            item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                        # item in suggestions list should not be the same as\n",
    "                        # the string itself\n",
    "                        assert sc_item != string\n",
    "\n",
    "                        # calculate edit distance using, for example,\n",
    "                        # Damerau-Levenshtein distance\n",
    "                        item_dist = dameraulevenshtein(sc_item, string)\n",
    "\n",
    "                        # do not add words with greater edit distance if\n",
    "                        # verbose setting not on\n",
    "                        if (self.verbose < 2) and (item_dist > min_suggest_len):\n",
    "                            pass\n",
    "                        elif item_dist <= self.max_edit_distance:\n",
    "                            assert sc_item in self.dictionary  # should already be in dictionary if in suggestion list\n",
    "                            suggest_dict[sc_item] = (self.dictionary[sc_item][1], item_dist)\n",
    "                            if item_dist < min_suggest_len:\n",
    "                                min_suggest_len = item_dist\n",
    "\n",
    "                        # depending on order words are processed, some words\n",
    "                        # with different edit distances may be entered into\n",
    "                        # suggestions; trim suggestion dictionary if verbose\n",
    "                        # setting not on\n",
    "                        if self.verbose < 2:\n",
    "                            suggest_dict = {k: v for k, v in suggest_dict.items() if v[1] <= min_suggest_len}\n",
    "\n",
    "            # now generate deletes (e.g. a substring of string or of a delete)\n",
    "            # from the queue item\n",
    "            # as additional items to check -- add to end of queue\n",
    "            assert len(string) >= len(q_item)\n",
    "\n",
    "            # do not add words with greater edit distance if verbose setting\n",
    "            # is not on\n",
    "            if (self.verbose < 2) and ((len(string) - len(q_item)) > min_suggest_len):\n",
    "                pass\n",
    "            elif (len(string) - len(q_item)) < self.max_edit_distance and len(q_item) > 1:\n",
    "                for c in range(len(q_item)):  # character index\n",
    "                    word_minus_c = q_item[:c] + q_item[c + 1:]\n",
    "                    if word_minus_c not in q_dictionary:\n",
    "                        queue.append(word_minus_c)\n",
    "                        q_dictionary[word_minus_c] = None  # arbitrary value, just to identify we checked this\n",
    "\n",
    "        # queue is now empty: convert suggestions in dictionary to\n",
    "        # list for output\n",
    "        if not silent and self.verbose != 0:\n",
    "            print(\"number of possible corrections: %i\" % len(suggest_dict))\n",
    "            print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
    "\n",
    "        # output option 1\n",
    "        # sort results by ascending order of edit distance and descending\n",
    "        # order of frequency\n",
    "        #     and return list of suggested word corrections only:\n",
    "        # return sorted(suggest_dict, key = lambda x:\n",
    "        #               (suggest_dict[x][1], -suggest_dict[x][0]))\n",
    "\n",
    "        # output option 2\n",
    "        # return list of suggestions with (correction,\n",
    "        #                                  (frequency in corpus, edit distance)):\n",
    "        as_list = suggest_dict.items()\n",
    "        # outlist = sorted(as_list, key=lambda (term, (freq, dist)): (dist, -freq))\n",
    "        outlist = sorted(as_list, key=lambda x: (x[1][1], -x[1][0]))\n",
    "\n",
    "        if self.verbose == 0:\n",
    "            return outlist[0]\n",
    "        else:\n",
    "            return outlist\n",
    "\n",
    "        '''\n",
    "        Option 1:\n",
    "        ['file', 'five', 'fire', 'fine', ...]\n",
    "        Option 2:\n",
    "        [('file', (5, 0)),\n",
    "         ('five', (67, 1)),\n",
    "         ('fire', (54, 1)),\n",
    "         ('fine', (17, 1))...]  \n",
    "        '''\n",
    "\n",
    "    def best_word(self, s, silent=False):\n",
    "        try:\n",
    "            return self.get_suggestions(s, silent)[0]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def spell_corrector(word_list, words_d) -> str:\n",
    "    result_list = []\n",
    "    for word in word_list:\n",
    "        if word not in words_d:\n",
    "            suggestion = ss.best_word(word, silent=True)\n",
    "            if suggestion is not None:\n",
    "                result_list.append(suggestion)\n",
    "        else:\n",
    "            result_list.append(word)\n",
    "            \n",
    "    return \" \".join(result_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # build symspell tree \n",
    "    ss = SymSpell(max_edit_distance=2)\n",
    "    \n",
    "    # fetch list of bad words\n",
    "    with open('bad-words.csv') as bf:\n",
    "        bad_words = bf.readlines()\n",
    "    bad_words = [word.strip() for word in bad_words]    \n",
    "    \n",
    "    # fetch english words dictionary\n",
    "    with open('english_words_479k.txt') as f:\n",
    "        words = f.readlines()\n",
    "    eng_words = [word.strip() for word in words]\n",
    "    \n",
    "    # Print some examples\n",
    "    print(eng_words[:5])\n",
    "    print(bad_words[:5])\n",
    "\n",
    "    print('Total english words: {}'.format(len(eng_words)))\n",
    "    print('Total bad words: {}'.format(len(bad_words)))\n",
    "    \n",
    "    print('create symspell dict...')\n",
    "    \n",
    "    if to_sample:\n",
    "        # sampling from list for kernel runtime\n",
    "        sample_idxs = random.sample(range(len(eng_words)), 100)\n",
    "        eng_words = [eng_words[i] for i in sorted(sample_idxs)] + \\\n",
    "            'to infinity and beyond'.split() # make sure our sample misspell is in there\n",
    "    \n",
    "    all_words_list = list(set(bad_words + eng_words))\n",
    "    silence = ss.create_dictionary_from_arr(all_words_list, token_pattern=r'.+')\n",
    "    \n",
    "    # create a dictionary of rightly spelled words for lookup\n",
    "    words_dict = {k: 0 for k in all_words_list}\n",
    "    \n",
    "    sample_text = 'to infifity and byond'\n",
    "    tokens = spacy_tokenize(sample_text)\n",
    "    \n",
    "    print('run spell checker...')\n",
    "    print()\n",
    "    print('original text: ' + sample_text)\n",
    "    print()\n",
    "    correct_text = spell_corrector(tokens, words_dict)\n",
    "    print('corrected text: ' + correct_text)\n",
    "\n",
    "    print('Done.')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
